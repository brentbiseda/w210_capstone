{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "#import torch\n",
    "#from transformers import AutoTokenizer, AutoModel\n",
    "#import re\n",
    "#import string\n",
    "import numpy as np\n",
    "#import pydot\n",
    "#from nltk.corpus import stopwords \n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "#from simpletransformers.classification import MultiLabelClassificationModel\n",
    "#import logging\n",
    "#import custom_sentence_tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy import stats\n",
    "#from ast import literal_eval\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import gc\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, Reshape\n",
    "from keras.layers import GlobalMaxPooling1D,Conv2D, Conv1D, AveragePooling2D, MaxPooling1D, AveragePooling1D, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "#from sklearn.decomposition import PCA\n",
    "import _pickle as cPickle\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Embeddings (X) and Labels (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.87386226654053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.time()\n",
    "infile = None\n",
    "infile = open(\"embeddings_evaluation\",'rb')\n",
    "embeddings_evaluation = cPickle.load(infile)\n",
    "infile.close()\n",
    "del(infile)\n",
    "print(time.time()-now)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.81072735786438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.time()\n",
    "infile = None\n",
    "infile = open(\"embeddings_7.1\",'rb')\n",
    "embeddings = cPickle.load(infile)\n",
    "infile.close()\n",
    "del(infile)\n",
    "print(time.time()-now)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.update(embeddings_evaluation)\n",
    "pickle.dump( embeddings, open( \"/home/ec2-user/SageMaker/embeddings.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(embeddings_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.77402520179749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.time()\n",
    "infile = None\n",
    "infile = open(\"embeddings.p\",'rb')\n",
    "embeddings = cPickle.load(infile)\n",
    "infile.close()\n",
    "del(infile)\n",
    "print(time.time()-now)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np_complete = np.array(list(embeddings.items()))\n",
    "del(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load chapter model and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load chapeter model weights\n",
    "def call_model(file_name):\n",
    "    model = None\n",
    "    json_file = open('/home/ec2-user/SageMaker/Models/model_'+file_name+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    opt = keras.optimizers.Adam(lr=0.001)\n",
    "    # load weights into new model\n",
    "    model.load_weights('/home/ec2-user/SageMaker/Models/model_'+file_name+\".h5\")\n",
    "    model.trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(embeddings):\n",
    "    max_sentence = 20\n",
    "    padding = max_sentence - len(embeddings)\n",
    "    if padding > 0:\n",
    "        padding_shape = (padding, 256, 768)\n",
    "        pad = np.zeros(padding*256*768).reshape(padding_shape)\n",
    "        return np.append(embeddings, pad, axis = 0).astype('float16')\n",
    "    else:\n",
    "        return embeddings\n",
    "    \n",
    "def split_for_train(HADM_ID_LIST, CHAPTER_PIVOT_DF, embeddings_concat):\n",
    "    HADM_ID_DF = None\n",
    "    loaded_model = None\n",
    "    HADM_ID_DF =  pd.DataFrame(data=HADM_ID_LIST, columns=['HADM_ID'])\n",
    "    CHAPTER_PIVOT_DF_COPY = HADM_ID_DF.set_index('HADM_ID')\\\n",
    "                    .join(CHAPTER_PIVOT_DF, how='left').copy(deep=False)\n",
    "    CHAPTER_PIVOT_NP = np.array(CHAPTER_PIVOT_DF_COPY)\n",
    "    return train_test_split(embeddings_concat, CHAPTER_PIVOT_NP, test_size=0.5)\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "import tensorflow as tf\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "    #p1 = tn / (tn + fn + K.epsilon())\n",
    "    #r1 = tn / (tn + fp + K.epsilon())\n",
    "    \n",
    "    f1 = p*r / (p+r+K.epsilon())\n",
    "    #+ p1*r1 / (p1+r1+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loop on individual therapy area (bag of models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============Started 001_139.csv============\n",
      "001_139.csv orginal shape: (5135, 2)\n",
      "001_139.csv Padding complete\n",
      "001_139.csv modified shape: (5135, 5120, 768)\n",
      "Train and Test Shapre: (2567, 5120, 768), (2568, 5120, 768), (2567, 4), (2568, 4)\n",
      "2568/2568 [==============================] - 44s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      1788\n",
      "           1       0.88      0.52      0.65       631\n",
      "           2       0.86      0.55      0.67       471\n",
      "           3       0.98      0.15      0.26       428\n",
      "\n",
      "   micro avg       0.86      0.72      0.79      3318\n",
      "   macro avg       0.90      0.55      0.62      3318\n",
      "weighted avg       0.88      0.72      0.74      3318\n",
      " samples avg       0.81      0.75      0.76      3318\n",
      "\n",
      "\n",
      "\n",
      "============Started 140_239.csv============\n",
      "140_239.csv orginal shape: (2369, 2)\n",
      "140_239.csv Padding complete\n",
      "140_239.csv modified shape: (2369, 5120, 768)\n",
      "Train and Test Shapre: (1184, 5120, 768), (1185, 5120, 768), (1184, 3), (1185, 3)\n",
      "1185/1185 [==============================] - 20s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78       405\n",
      "           1       0.92      0.98      0.95       921\n",
      "           2       0.93      0.73      0.82       264\n",
      "\n",
      "   micro avg       0.91      0.86      0.89      1590\n",
      "   macro avg       0.91      0.80      0.85      1590\n",
      "weighted avg       0.91      0.86      0.88      1590\n",
      " samples avg       0.91      0.89      0.89      1590\n",
      "\n",
      "\n",
      "\n",
      "============Started 240_279.csv============\n",
      "240_279.csv orginal shape: (6263, 2)\n",
      "240_279.csv Padding complete\n",
      "240_279.csv modified shape: (6263, 5120, 768)\n",
      "Train and Test Shapre: (3131, 5120, 768), (3132, 5120, 768), (3131, 4), (3132, 4)\n",
      "3132/3132 [==============================] - 53s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.55      0.67       691\n",
      "           1       0.83      0.94      0.88      2030\n",
      "           2       0.84      0.61      0.70      1030\n",
      "           3       0.95      0.23      0.37       549\n",
      "\n",
      "   micro avg       0.84      0.71      0.77      4300\n",
      "   macro avg       0.87      0.58      0.66      4300\n",
      "weighted avg       0.86      0.71      0.74      4300\n",
      " samples avg       0.78      0.73      0.74      4300\n",
      "\n",
      "\n",
      "\n",
      "============Started 280_289.csv============\n",
      "280_289.csv orginal shape: (6212, 2)\n",
      "280_289.csv Padding complete\n",
      "280_289.csv modified shape: (6212, 5120, 768)\n",
      "Train and Test Shapre: (3106, 5120, 768), (3106, 5120, 768), (3106, 3), (3106, 3)\n",
      "3106/3106 [==============================] - 54s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.85      1595\n",
      "           1       0.88      0.77      0.82      1202\n",
      "           2       0.99      0.24      0.39       369\n",
      "\n",
      "   micro avg       0.84      0.77      0.80      3166\n",
      "   macro avg       0.89      0.63      0.69      3166\n",
      "weighted avg       0.86      0.77      0.78      3166\n",
      " samples avg       0.76      0.75      0.75      3166\n",
      "\n",
      "\n",
      "\n",
      "============Started 290_319.csv============\n",
      "290_319.csv orginal shape: (5661, 2)\n",
      "290_319.csv Padding complete\n",
      "290_319.csv modified shape: (5661, 5120, 768)\n",
      "Train and Test Shapre: (2830, 5120, 768), (2831, 5120, 768), (2830, 4), (2831, 4)\n",
      "2831/2831 [==============================] - 49s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.41      0.57       423\n",
      "           1       0.87      0.56      0.68      1039\n",
      "           2       0.87      0.59      0.70      1089\n",
      "           3       0.90      0.56      0.69       762\n",
      "\n",
      "   micro avg       0.88      0.55      0.68      3313\n",
      "   macro avg       0.89      0.53      0.66      3313\n",
      "weighted avg       0.88      0.55      0.67      3313\n",
      " samples avg       0.57      0.52      0.54      3313\n",
      "\n",
      "\n",
      "\n",
      "============Started 320_389.csv============\n",
      "320_389.csv orginal shape: (3834, 2)\n",
      "320_389.csv Padding complete\n",
      "320_389.csv modified shape: (3834, 5120, 768)\n",
      "Train and Test Shapre: (1917, 5120, 768), (1917, 5120, 768), (1917, 4), (1917, 4)\n",
      "1917/1917 [==============================] - 33s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.65      0.76       684\n",
      "           1       0.93      0.51      0.66       372\n",
      "           2       0.78      0.76      0.77       342\n",
      "           3       0.92      0.55      0.69       602\n",
      "\n",
      "   micro avg       0.88      0.61      0.72      2000\n",
      "   macro avg       0.88      0.62      0.72      2000\n",
      "weighted avg       0.89      0.61      0.72      2000\n",
      " samples avg       0.59      0.56      0.57      2000\n",
      "\n",
      "\n",
      "\n",
      "============Started 390_459.csv============\n",
      "390_459.csv orginal shape: (10831, 2)\n",
      "390_459.csv Padding complete\n",
      "390_459.csv modified shape: (10831, 5120, 768)\n",
      "Train and Test Shapre: (5415, 5120, 768), (5416, 5120, 768), (5415, 6), (5416, 6)\n",
      "5416/5416 [==============================] - 92s 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.54      0.63      1660\n",
      "           1       0.76      0.47      0.58      1948\n",
      "           2       0.84      0.62      0.71      2033\n",
      "           3       0.65      0.72      0.68      2328\n",
      "           4       0.70      0.26      0.38      1154\n",
      "           5       0.72      0.28      0.40       831\n",
      "\n",
      "   micro avg       0.73      0.53      0.61      9954\n",
      "   macro avg       0.74      0.48      0.56      9954\n",
      "weighted avg       0.74      0.53      0.60      9954\n",
      " samples avg       0.56      0.48      0.49      9954\n",
      "\n",
      "\n",
      "\n",
      "============Started 460_519.csv============\n",
      "460_519.csv orginal shape: (9021, 2)\n",
      "460_519.csv Padding complete\n",
      "460_519.csv modified shape: (9021, 5120, 768)\n",
      "Train and Test Shapre: (4510, 5120, 768), (4511, 5120, 768), (4510, 5), (4511, 5)\n",
      "4511/4511 [==============================] - 80s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73      1808\n",
      "           1       0.80      0.26      0.39       891\n",
      "           2       0.91      0.05      0.09       628\n",
      "           3       0.79      0.41      0.54      1495\n",
      "           4       0.78      0.62      0.69      1567\n",
      "\n",
      "   micro avg       0.79      0.48      0.60      6389\n",
      "   macro avg       0.82      0.40      0.49      6389\n",
      "weighted avg       0.80      0.48      0.57      6389\n",
      " samples avg       0.57      0.48      0.50      6389\n",
      "\n",
      "\n",
      "\n",
      "============Started 520_579.csv============\n",
      "520_579.csv orginal shape: (4512, 2)\n",
      "520_579.csv Padding complete\n",
      "520_579.csv modified shape: (4512, 5120, 768)\n",
      "Train and Test Shapre: (2256, 5120, 768), (2256, 5120, 768), (2256, 5), (2256, 5)\n",
      "2256/2256 [==============================] - 40s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.51      0.62       443\n",
      "           1       0.80      0.81      0.80      1027\n",
      "           2       0.93      0.17      0.29       300\n",
      "           3       0.77      0.82      0.79       651\n",
      "           4       0.90      0.09      0.16       309\n",
      "\n",
      "   micro avg       0.79      0.61      0.69      2730\n",
      "   macro avg       0.84      0.48      0.54      2730\n",
      "weighted avg       0.82      0.61      0.64      2730\n",
      " samples avg       0.65      0.62      0.62      2730\n",
      "\n",
      "\n",
      "\n",
      "============Started 580_629.csv============\n",
      "580_629.csv orginal shape: (8339, 2)\n",
      "580_629.csv Padding complete\n",
      "580_629.csv modified shape: (8339, 5120, 768)\n",
      "Train and Test Shapre: (4169, 5120, 768), (4170, 5120, 768), (4169, 3), (4170, 3)\n",
      "4170/4170 [==============================] - 74s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.65      0.74      1387\n",
      "           1       0.88      0.91      0.89      2714\n",
      "           2       0.80      0.68      0.73      1586\n",
      "\n",
      "   micro avg       0.85      0.78      0.82      5687\n",
      "   macro avg       0.85      0.75      0.79      5687\n",
      "weighted avg       0.85      0.78      0.81      5687\n",
      " samples avg       0.83      0.81      0.80      5687\n",
      "\n",
      "\n",
      "\n",
      "============Started 630_679__740_759__760_779.csv============\n",
      "630_679__740_759__760_779.csv orginal shape: (1852, 2)\n",
      "630_679__740_759__760_779.csv Padding complete\n",
      "630_679__740_759__760_779.csv modified shape: (1852, 5120, 768)\n",
      "Train and Test Shapre: (926, 5120, 768), (926, 5120, 768), (926, 4), (926, 4)\n",
      "926/926 [==============================] - 21s 23ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89       662\n",
      "           1       0.70      0.31      0.43       219\n",
      "           2       0.73      0.35      0.47       300\n",
      "           3       0.78      0.83      0.80       393\n",
      "\n",
      "   micro avg       0.79      0.73      0.76      1574\n",
      "   macro avg       0.75      0.62      0.65      1574\n",
      "weighted avg       0.77      0.73      0.73      1574\n",
      " samples avg       0.70      0.67      0.65      1574\n",
      "\n",
      "\n",
      "\n",
      "============Started 710-739.csv============\n",
      "710-739.csv orginal shape: (2197, 2)\n",
      "710-739.csv Padding complete\n",
      "710-739.csv modified shape: (2197, 5120, 768)\n",
      "Train and Test Shapre: (1098, 5120, 768), (1099, 5120, 768), (1098, 2), (1099, 2)\n",
      "1099/1099 [==============================] - 19s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.72      0.81       459\n",
      "           1       0.86      0.95      0.90       685\n",
      "\n",
      "   micro avg       0.88      0.86      0.87      1144\n",
      "   macro avg       0.89      0.83      0.86      1144\n",
      "weighted avg       0.89      0.86      0.86      1144\n",
      " samples avg       0.87      0.86      0.86      1144\n",
      "\n",
      "\n",
      "\n",
      "============Started 780-799.csv============\n",
      "780-799.csv orginal shape: (3784, 2)\n",
      "780-799.csv Padding complete\n",
      "780-799.csv modified shape: (3784, 5120, 768)\n",
      "Train and Test Shapre: (1892, 5120, 768), (1892, 5120, 768), (1892, 5), (1892, 5)\n",
      "1892/1892 [==============================] - 58s 31ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.54      0.67       337\n",
      "           1       0.90      0.32      0.48       405\n",
      "           2       0.81      0.67      0.73       375\n",
      "           3       0.85      0.37      0.52       354\n",
      "           4       0.79      0.77      0.78       505\n",
      "\n",
      "   micro avg       0.83      0.55      0.66      1976\n",
      "   macro avg       0.85      0.54      0.64      1976\n",
      "weighted avg       0.85      0.55      0.64      1976\n",
      " samples avg       0.55      0.53      0.53      1976\n",
      "\n",
      "\n",
      "\n",
      "============Started 800-999.csv============\n",
      "800-999.csv orginal shape: (4454, 2)\n",
      "800-999.csv Padding complete\n",
      "800-999.csv modified shape: (4454, 5120, 768)\n",
      "Train and Test Shapre: (2227, 5120, 768), (2227, 5120, 768), (2227, 4), (2227, 4)\n",
      "2227/2227 [==============================] - 39s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.56      0.70       304\n",
      "           1       0.88      0.55      0.68       308\n",
      "           2       0.93      0.53      0.67       391\n",
      "           3       0.88      0.90      0.89      1139\n",
      "\n",
      "   micro avg       0.90      0.73      0.80      2142\n",
      "   macro avg       0.91      0.63      0.73      2142\n",
      "weighted avg       0.90      0.73      0.79      2142\n",
      " samples avg       0.68      0.67      0.67      2142\n",
      "\n",
      "\n",
      "\n",
      "============Started E_V.csv============\n",
      "E_V.csv orginal shape: (5124, 2)\n",
      "E_V.csv Padding complete\n",
      "E_V.csv modified shape: (5124, 5120, 768)\n",
      "Train and Test Shapre: (2562, 5120, 768), (2562, 5120, 768), (2562, 7), (2562, 7)\n",
      "2562/2562 [==============================] - 45s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.70      0.74       532\n",
      "           1       0.79      0.77      0.78       651\n",
      "           2       0.86      0.61      0.71       569\n",
      "           3       0.75      0.98      0.85       727\n",
      "           4       0.62      1.00      0.76       622\n",
      "           5       0.84      0.06      0.12       407\n",
      "           6       0.00      0.00      0.00       264\n",
      "\n",
      "   micro avg       0.74      0.68      0.71      3772\n",
      "   macro avg       0.66      0.59      0.57      3772\n",
      "weighted avg       0.71      0.68      0.65      3772\n",
      " samples avg       0.70      0.69      0.68      3772\n",
      "\n",
      "\n",
      "\n",
      "============Started model_adverse_effect_2.csv============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File Layer-2/model_adverse_effect_2.csv does not exist: 'Layer-2/model_adverse_effect_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-453e6aeb62d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_name_LIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n============Started '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'============'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mCHAPTER_PIVOT_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Layer-2/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'HADM_ID'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HADM_ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mHADM_ID_LIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHAPTER_PIVOT_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File Layer-2/model_adverse_effect_2.csv does not exist: 'Layer-2/model_adverse_effect_2.csv'"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "history_list = []\n",
    "file_name_LIST = ['001_139.csv','140_239.csv','240_279.csv','280_289.csv','290_319.csv','320_389.csv','390_459.csv'\n",
    ",'460_519.csv','520_579.csv','580_629.csv','630_679__740_759__760_779.csv', '710-739.csv','780-799.csv','800-999.csv',\n",
    "                  'E_V.csv', 'model_adverse_effect_2.csv']\n",
    "1. #file_name_LIST = ['001_139.csv','140_239.csv','240_279.csv','800-999.csv']\n",
    "#file_name_LIST = ['320_389.csv']\n",
    "#file_name_LIST = ['630_679__740_759__760_779.csv']\n",
    "\n",
    "for file_name in file_name_LIST:\n",
    "    print('\\n\\n============Started '+file_name+'============')\n",
    "    CHAPTER_PIVOT_DF = pd.read_csv('Layer-2/'+file_name, sep=',', header = 0)\\\n",
    "                        .astype({'HADM_ID': 'str'}).set_index('HADM_ID')\n",
    "    HADM_ID_LIST = list(CHAPTER_PIVOT_DF.index)\n",
    "    embeddings_np = embeddings_np_complete[np.isin(embeddings_np_complete[:,0], HADM_ID_LIST)]\n",
    "    print(file_name+' orginal shape: '+str(embeddings_np.shape))\n",
    "    HADM_ID_LIST = embeddings_np[:,0].copy()\n",
    "    embeddings = embeddings_np[:,1].copy()\n",
    "    \n",
    "    #Zero Pad embeddings\n",
    "    embeddings_padded = [padding(embeddings_ITEM) for embeddings_ITEM in embeddings]\n",
    "    del(embeddings)\n",
    "    gc.collect()\n",
    "    print(file_name+' Padding complete')\n",
    "\n",
    "    #Reshape embeddings\n",
    "    #embeddings_concat = np.array([np.concatenate(i) for i in embeddings_padded])\n",
    "    embeddings_concat = np.array(embeddings_padded).reshape(len(embeddings_padded), 20*256, 768)\n",
    "    print(file_name+' modified shape: '+str(embeddings_concat.shape))\n",
    "    del(embeddings_padded)\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train2, X_test2, y_train2, y_test2 = split_for_train(HADM_ID_LIST, CHAPTER_PIVOT_DF, embeddings_concat)\n",
    "    print('Train and Test Shapre: '+str(X_train2.shape)+', '+str(X_test2.shape)+', '+str(y_train2.shape)+', '+str(y_test2.shape))\n",
    "    X_val = X_test2[:int(0.25*len(X_test2))]\n",
    "    y_val = y_test2[:int(0.25*len(y_test2))]\n",
    "    del(HADM_ID_LIST)\n",
    "    del(CHAPTER_PIVOT_DF)\n",
    "    del(embeddings_concat)\n",
    "    gc.collect()\n",
    "    model = call_model(file_name)\n",
    "    opt = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1, 'accuracy'])\n",
    "    predict = model.predict(X_test2, verbose=1)\n",
    "    print(classification_report(y_test2, np.where(predict > 0.5, 1, 0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============Started adverse_effect_2.csv============\n",
      "adverse_effect_2.csv orginal shape: (4962, 2)\n",
      "adverse_effect_2.csv Padding complete\n",
      "adverse_effect_2.csv modified shape: (4962, 5120, 768)\n",
      "Train and Test Shapre: (2481, 5120, 768), (2481, 5120, 768), (2481, 5), (2481, 5)\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "history_list = []\n",
    "file_name_LIST = ['adverse_effect_2.csv']\n",
    "1. #file_name_LIST = ['001_139.csv','140_239.csv','240_279.csv','800-999.csv']\n",
    "#file_name_LIST = ['320_389.csv']\n",
    "#file_name_LIST = ['630_679__740_759__760_779.csv']\n",
    "\n",
    "for file_name in file_name_LIST:\n",
    "    print('\\n\\n============Started '+file_name+'============')\n",
    "    CHAPTER_PIVOT_DF = pd.read_csv('Layer-2/'+file_name, sep=',', header = 0)\\\n",
    "                        .astype({'HADM_ID': 'str'}).set_index('HADM_ID')\n",
    "    HADM_ID_LIST = list(CHAPTER_PIVOT_DF.index)\n",
    "    embeddings_np = embeddings_np_complete[np.isin(embeddings_np_complete[:,0], HADM_ID_LIST)]\n",
    "    print(file_name+' orginal shape: '+str(embeddings_np.shape))\n",
    "    HADM_ID_LIST = embeddings_np[:,0].copy()\n",
    "    embeddings = embeddings_np[:,1].copy()\n",
    "    \n",
    "    #Zero Pad embeddings\n",
    "    embeddings_padded = [padding(embeddings_ITEM) for embeddings_ITEM in embeddings]\n",
    "    del(embeddings)\n",
    "    gc.collect()\n",
    "    print(file_name+' Padding complete')\n",
    "\n",
    "    #Reshape embeddings\n",
    "    #embeddings_concat = np.array([np.concatenate(i) for i in embeddings_padded])\n",
    "    embeddings_concat = np.array(embeddings_padded).reshape(len(embeddings_padded), 20*256, 768)\n",
    "    print(file_name+' modified shape: '+str(embeddings_concat.shape))\n",
    "    del(embeddings_padded)\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train2, X_test2, y_train2, y_test2 = split_for_train(HADM_ID_LIST, CHAPTER_PIVOT_DF, embeddings_concat)\n",
    "    print('Train and Test Shapre: '+str(X_train2.shape)+', '+str(X_test2.shape)+', '+str(y_train2.shape)+', '+str(y_test2.shape))\n",
    "    X_val = X_test2[:int(0.25*len(X_test2))]\n",
    "    y_val = y_test2[:int(0.25*len(y_test2))]\n",
    "    del(HADM_ID_LIST)\n",
    "    del(CHAPTER_PIVOT_DF)\n",
    "    del(embeddings_concat)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2481/2481 [==============================] - 39s 16ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.61      0.69       503\n",
      "           1       0.67      0.81      0.73       890\n",
      "           2       0.78      0.72      0.75      1012\n",
      "           3       0.79      0.69      0.74       812\n",
      "           4       0.68      0.68      0.68       650\n",
      "\n",
      "   micro avg       0.73      0.72      0.72      3867\n",
      "   macro avg       0.74      0.70      0.72      3867\n",
      "weighted avg       0.74      0.72      0.72      3867\n",
      " samples avg       0.72      0.73      0.70      3867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = call_model(file_name)\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1, 'accuracy'])\n",
    "predict = model.predict(X_test2, verbose=1)\n",
    "print(classification_report(y_test2, np.where(predict > 0.5, 1, 0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
