{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept - Predict Top-5 diagnosis categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul> <li> D1: Diseases of the circulatory system </ul> </li>\n",
    "<ul> <li> D2: External causes of injury and supplemental classification </ul> </li>\n",
    "<ul> <li> D3: Endocrine, nutritional and metabolic diseases, and immunity disorders </ul> </li>\n",
    "<ul> <li> D4: Diseases of the respiratory system </ul> </li>\n",
    "<ul> <li> D5: Injury and poisoning </ul> </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre process data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import collect_list\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, VectorAssembler, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"ClickThrough\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the S3 URL as per your config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_PREFIX = 's3://'\n",
    "S3_BUCKET = '<bucket name>'\n",
    "S3_PATH = '<folder path>'\n",
    "S3_URL = S3_PREFIX + S3_BUCKET + S3_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_LIST = ['D_ICD_DIAGNOSES.csv','DIAGNOSES_ICD.csv','NOTEEVENTS.csv']\n",
    "for filename_LIST_ITEM in filename_LIST:\n",
    "    filename,fileformat = filename_LIST_ITEM.split('.')\n",
    "    exec(filename+'_DF = sqlContext.read.format(\"'+fileformat+'\").option(\"header\", \"true\").option(\"multiline\",True).'+\n",
    "         'option(\"escape\",'+\"'\"+'\"'+\"')\"+'.load(\"'+S3_URL+'/'\n",
    "         +filename+'.csv\")')\n",
    "    exec(filename+'_DF.createOrReplaceTempView(\"'+filename+'\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Build HADM level diagnosis flag tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select A.*, B.SHORT_TITLE, B.LONG_TITLE, \n",
    "case when substr(A.ICD9_CODE,1,1) in ('E','V') then 'external causes of injury and supplemental classification'\n",
    "when substr(A.ICD9_CODE,1,3) between 001 and 139 then 'infectious and parasitic diseases'\n",
    "when substr(A.ICD9_CODE,1,3) between 140 and 239 then 'neoplasms'\n",
    "when substr(A.ICD9_CODE,1,3) between 240 and 279 then 'endocrine, nutritional and metabolic diseases, and immunity disorders'\n",
    "when substr(A.ICD9_CODE,1,3) between 280 and 289 then 'diseases of the blood and blood-forming organs'\n",
    "when substr(A.ICD9_CODE,1,3) between 290 and 319 then 'mental disorders'\n",
    "when substr(A.ICD9_CODE,1,3) between 320 and 389 then 'diseases of the nervous system and sense organs'\n",
    "when substr(A.ICD9_CODE,1,3) between 390 and 459 then 'diseases of the circulatory system'\n",
    "when substr(A.ICD9_CODE,1,3) between 460 and 519 then 'diseases of the respiratory system'\n",
    "when substr(A.ICD9_CODE,1,3) between 520 and 579 then 'diseases of the digestive system'\n",
    "when substr(A.ICD9_CODE,1,3) between 580 and 629 then 'diseases of the genitourinary system'\n",
    "when substr(A.ICD9_CODE,1,3) between 630 and 679 then 'complications of pregnancy, childbirth, and the puerperium'\n",
    "when substr(A.ICD9_CODE,1,3) between 680 and 709 then 'diseases of the skin and subcutaneous tissue'\n",
    "when substr(A.ICD9_CODE,1,3) between 710 and 739 then 'diseases of the musculoskeletal system and connective tissue'\n",
    "when substr(A.ICD9_CODE,1,3) between 740 and 759 then 'congenital anomalies'\n",
    "when substr(A.ICD9_CODE,1,3) between 760 and 779 then 'certain conditions originating in the perinatal period'\n",
    "when substr(A.ICD9_CODE,1,3) between 780 and 799 then 'symptoms, signs, and ill-defined conditions'\n",
    "when substr(A.ICD9_CODE,1,3) between 800 and 999 then 'injury and poisoning' \n",
    "end as ICD_GROUP\n",
    "from DIAGNOSES_ICD A\n",
    "left join D_ICD_DIAGNOSES B\n",
    "on A.ICD9_CODE = B.ICD9_CODE\n",
    "\"\"\").createOrReplaceTempView('DIAGNOSES_ICD_WITH_GROUPING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGNOSIS_GROUPING_DF = spark.sql(\"\"\"select distinct HADM_ID, \n",
    "case when ICD_GROUP = 'diseases of the circulatory system' then 'D1'\n",
    "when ICD_GROUP = 'external causes of injury and supplemental classification' then 'D2'\n",
    "when ICD_GROUP = 'endocrine, nutritional and metabolic diseases, and immunity disorders' then 'D3'\n",
    "when ICD_GROUP = 'diseases of the respiratory system' then 'D4'\n",
    "when ICD_GROUP = 'injury and poisoning' then 'D5' end as ICD_GROUP_ID\n",
    "from DIAGNOSES_ICD_WITH_GROUPING \n",
    "where ICD_GROUP in ('diseases of the circulatory system',\n",
    "'external causes of injury and supplemental classification',\n",
    "'endocrine, nutritional and metabolic diseases, and immunity disorders',\n",
    "'diseases of the respiratory system',\n",
    "'injury and poisoning')\"\"\")\n",
    "DIAGNOSIS_GROUPING_PIVOT_DF = DIAGNOSIS_GROUPING_DF.groupBy('HADM_ID').pivot('ICD_GROUP_ID').count().fillna(0)\n",
    "DIAGNOSIS_GROUPING_PIVOT_DF.createOrReplaceTempView('DIAGNOSIS_GROUPING_PIVOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+\n",
      "|HADM_ID| D1| D2| D3| D4| D5|\n",
      "+-------+---+---+---+---+---+\n",
      "| 153296|  1|  0|  1|  0|  0|\n",
      "| 109960|  1|  1|  0|  0|  0|\n",
      "| 155280|  1|  1|  1|  1|  0|\n",
      "| 144037|  1|  1|  1|  0|  0|\n",
      "| 149907|  0|  1|  0|  1|  1|\n",
      "+-------+---+---+---+---+---+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from DIAGNOSIS_GROUPING_PIVOT limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+\n",
      "|HADM_ID| D1| D2| D3| D4| D5|\n",
      "+-------+---+---+---+---+---+\n",
      "| 166435|  1|  1|  1|  0|  1|\n",
      "| 125592|  1|  1|  1|  0|  1|\n",
      "| 100263|  1|  1|  1|  1|  1|\n",
      "| 179104|  1|  1|  1|  0|  0|\n",
      "| 118388|  1|  0|  1|  1|  0|\n",
      "+-------+---+---+---+---+---+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "DIAGNOSIS_GROUPING_PIVOT_DF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Combine all notes at HADM level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_DF = spark.sql(\"\"\"SELECT A.HADM_ID, B.D1, B.D2, B.D3, B.D4, B.D5, lower(A.TEXT) as TEXT_LOWER \n",
    "from NOTEEVENTS A inner join DIAGNOSIS_GROUPING_PIVOT B on A.HADM_ID = B.HADM_ID\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Group all Text at HADM level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_GROUPED_DF = NOTEEVENTS_DF.groupby('HADM_ID','D1','D2','D3','D4','D5') \\\n",
    "                        .agg(F.concat_ws(\"\", F.collect_list(NOTEEVENTS_DF.TEXT_LOWER)).alias('TEXT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+--------------------+\n",
      "|HADM_ID| D1| D2| D3| D4| D5|                TEXT|\n",
      "+-------+---+---+---+---+---+--------------------+\n",
      "| 100010|  0|  0|  1|  0|  0|admission date:  ...|\n",
      "| 100140|  0|  1|  0|  1|  0|[**2117-6-17**] 1...|\n",
      "| 100227|  1|  1|  0|  1|  1|admission date:  ...|\n",
      "| 100263|  1|  1|  1|  1|  1|admission date:  ...|\n",
      "| 100320|  1|  1|  1|  0|  0|admission date:  ...|\n",
      "+-------+---+---+---+---+---+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "NOTEEVENTS_GROUPED_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HADM_ID: string (nullable = true)\n",
      " |-- D1: long (nullable = true)\n",
      " |-- D2: long (nullable = true)\n",
      " |-- D3: long (nullable = true)\n",
      " |-- D4: long (nullable = true)\n",
      " |-- D5: long (nullable = true)\n",
      " |-- TEXT: string (nullable = false)"
     ]
    }
   ],
   "source": [
    "NOTEEVENTS_GROUPED_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  ...                                                  4\n",
      "summary  count  ...                                                max\n",
      "HADM_ID  57632  ...                                             199999\n",
      "D1       57632  ...                                                  1\n",
      "D2       57632  ...                                                  1\n",
      "D3       57632  ...                                                  1\n",
      "D4       57632  ...                                                  1\n",
      "D5       57632  ...                                                  1\n",
      "TEXT     57632  ...  y\\nname:  [**known lastname 95474**], [**known...\n",
      "\n",
      "[8 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "NOTEEVENTS_GROUPED_DF.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Convert to Parquete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS_GROUPED_DF.write.mode('overwrite').parquet(S3_URL+'/NOTEEVENTS_GROUPED_DIAG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Replace Anonymized Names and dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTES_RDD = NOTEEVENTS_GROUPED_DF.select('HADM_ID','D1','D2','D3','D4','D5','TEXT').rdd\n",
    "def ReplaceAnonym(x):\n",
    "    return (x[0], x[1], x[2], x[3], x[4], x[5],\n",
    "            16*x[1]+8*x[2]+4*x[3]+2*x[4]+1*x[5],\n",
    "            re.sub('\\n',' ',re.sub(r'\\[\\*\\*.+\\*\\*\\]','xxx',x[6])))\n",
    "NOTES_GROUPED_ICD_DF = NOTES_RDD.map(ReplaceAnonym).toDF(['HADM_ID','D1','D2','D3','D4','D5','DX_INDEX','TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+--------+--------------------+\n",
      "|HADM_ID| D1| D2| D3| D4| D5|DX_INDEX|                TEXT|\n",
      "+-------+---+---+---+---+---+--------+--------------------+\n",
      "| 100010|  0|  0|  1|  0|  0|       4|admission date:  ...|\n",
      "| 100140|  0|  1|  0|  1|  0|      10|xxx 12:28 pm  che...|\n",
      "| 100227|  1|  1|  0|  1|  1|      27|admission date:  ...|\n",
      "| 100263|  1|  1|  1|  1|  1|      31|admission date:  ...|\n",
      "| 100320|  1|  1|  1|  0|  0|       7|admission date:  ...|\n",
      "+-------+---+---+---+---+---+--------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "NOTES_GROUPED_ICD_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_LIST = ['did', \"couldn't\", 'herself', 'above', 'hers', 'ain', 'if', 'until', 'me', 'through', 'some', 'be', 'myself', 'because', 'don', \"shouldn't\", 'here', 'as', 'can', 'it', 'on', 'no', \"you've\", 'the', 'our', 'we', \"that'll\", 'do', 'then', 'will', 'most', 'yours', 'yourselves', 'he', 'yourself', 'few', 'with', 'mightn', 'doesn', 'at', 'y', 'only', \"you're\", 'down', 'how', 'any', 'very', 'wouldn', 'himself', \"hasn't\", 'll', 'm', 'its', 'off', 'themselves', 'other', 'own', 'are', 'from', 'just', 'itself', 'has', 'ourselves', 'each', 'which', 'weren', 'i', 'should', 'shan', 'having', 'those', 'have', 'than', 'or', 'there', 'were', 'up', \"mustn't\", \"wouldn't\", \"needn't\", 'was', 'why', 're', 'they', \"you'd\", 'she', 'her', 'isn', \"you'll\", 'under', 'shouldn', 'to', 'nor', 'and', 'd', 'my', 'o', 'a', 'by', 'after', 'against', 'your', 'does', \"it's\", 's', 'you', \"should've\", 'him', 'hasn', 'again', \"aren't\", 'into', 'where', 'couldn', 'below', 'ma', 'didn', 'ours', 'wasn', 'about', 'what', 'when', 'same', 'is', \"don't\", 'during', 'in', 'mustn', 'needn', 'had', 'while', 'too', 'both', 'but', 'whom', 'between', \"isn't\", 'theirs', 'won', 'out', 'an', 'that', 'for', \"didn't\", 'this', \"doesn't\", \"wasn't\", 'am', 'aren', 'these', 't', 'who', 'further', \"hadn't\", 'his', 'more', 'before', 'them', 'not', \"shan't\", \"mightn't\", 'such', 'been', 'haven', 'being', 'over', 'once', 've', 'hadn', 'doing', 'of', \"won't\", 'now', 'all', \"haven't\", 'their', \"weren't\", 'so', \"she's\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Count Vectorize TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"TEXT\", outputCol=\"WORDS\", pattern=\"\\\\W\")\n",
    "# remove stopwords\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"WORDS\", outputCol=\"WORDS_FILT\").setStopWords(stopword_LIST)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"WORDS_FILT\", outputCol=\"FEATURES\", vocabSize=5000, minDF=5)\n",
    "#combine all flags D1 thru D2 into a Dense vector\n",
    "assembler = VectorAssembler(inputCols = ['D1','D2','D3','D4','D5'], outputCol = 'DX_VEC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, assembler])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(NOTES_GROUPED_ICD_DF)\n",
    "NOTES_DX_DF = pipelineFit.transform(NOTES_GROUPED_ICD_DF)\\\n",
    "                .select('HADM_ID','D1','D2','D3','D4','D5','DX_INDEX','DX_VEC','FEATURES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+---+---+--------+---------------------+\n",
      "|HADM_ID|D1 |D2 |D3 |D4 |D5 |DX_INDEX|DX_VEC               |\n",
      "+-------+---+---+---+---+---+--------+---------------------+\n",
      "|100010 |0  |0  |1  |0  |0  |4       |(5,[2],[1.0])        |\n",
      "|100140 |0  |1  |0  |1  |0  |10      |(5,[1,3],[1.0,1.0])  |\n",
      "|100227 |1  |1  |0  |1  |1  |27      |[1.0,1.0,0.0,1.0,1.0]|\n",
      "|100263 |1  |1  |1  |1  |1  |31      |[1.0,1.0,1.0,1.0,1.0]|\n",
      "|100320 |1  |1  |1  |0  |0  |7       |[1.0,1.0,1.0,0.0,0.0]|\n",
      "+-------+---+---+---+---+---+--------+---------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "NOTES_DX_DF.select('HADM_ID','D1','D2','D3','D4','D5','DX_INDEX','DX_VEC').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+\n",
      "|HADM_ID|DX_INDEX|            FEATURES|\n",
      "+-------+--------+--------------------+\n",
      "| 100010|       4|(5000,[0,1,2,4,5,...|\n",
      "| 100140|      10|(5000,[0,1,3,4,5,...|\n",
      "| 100227|      27|(5000,[0,1,2,3,4,...|\n",
      "| 100263|      31|(5000,[0,1,2,3,4,...|\n",
      "| 100320|      28|(5000,[0,1,2,3,4,...|\n",
      "+-------+--------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "NOTES_DX_LR_DF = NOTES_DX_DF.select('HADM_ID','DX_INDEX','FEATURES').cache()\n",
    "NOTES_DX_LR_DF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Partition Training & Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 46177\n",
      "Test Dataset Count: 11455"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = NOTES_DX_DF.randomSplit([0.8, 0.2], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Logistic Regression using Count Vector Features for combination of 5 therapy areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=50, featuresCol='FEATURES', labelCol='DX_INDEX')\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_DF = predictions.select('HADM_ID','DX_INDEX','prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.06%"
     ]
    }
   ],
   "source": [
    "correct_combo = predictions_DF[predictions_DF['DX_INDEX']==predictions_DF['prediction']]['DX_INDEX'].count()\n",
    "total_test_records = predictions_DF['DX_INDEX'].count()\n",
    "print(str(round(100*correct_combo/total_test_records,2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Simple LR to predict individual therapy areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=50, featuresCol='FEATURES', labelCol='D1')\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions_D1 = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=50, featuresCol='FEATURES', labelCol='D2')\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions_D2 = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=50, featuresCol='FEATURES', labelCol='D3')\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions_D3 = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=50, featuresCol='FEATURES', labelCol='D4')\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions_D4 = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=50, featuresCol='FEATURES', labelCol='D5')\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions_D5 = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1 accuracy = 89.31%"
     ]
    }
   ],
   "source": [
    "predictions_DF1 = predictions_D1.select('D1','D2','D3','D4','D5','prediction').toPandas()\n",
    "correct_combo = predictions_DF1[predictions_DF1['D1']==predictions_DF1['prediction']]['prediction'].count()\n",
    "total_test_records = predictions_DF1['prediction'].count()\n",
    "print('D1 accuracy = '+str(round(100*correct_combo/total_test_records,2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction   0.0   1.0\n",
      "D1                    \n",
      "0           2394   664\n",
      "1            560  7837"
     ]
    }
   ],
   "source": [
    "pd.crosstab(predictions_DF1.D1,predictions_DF1.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2 accuracy = 76.06%"
     ]
    }
   ],
   "source": [
    "predictions_DF2 = predictions_D2.select('D1','D2','D3','D4','D5','prediction').toPandas()\n",
    "correct_combo = predictions_DF2[predictions_DF2['D2']==predictions_DF2['prediction']]['prediction'].count()\n",
    "total_test_records = predictions_DF2['prediction'].count()\n",
    "print('D2 accuracy = '+str(round(100*correct_combo/total_test_records,2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction   0.0   1.0\n",
      "D2                    \n",
      "0           1583  1634\n",
      "1           1108  7130"
     ]
    }
   ],
   "source": [
    "pd.crosstab(predictions_DF2.D2,predictions_DF2.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D3 accuracy = 79.3%"
     ]
    }
   ],
   "source": [
    "predictions_DF3 = predictions_D3.select('D1','D2','D3','D4','D5','prediction').toPandas()\n",
    "correct_combo = predictions_DF3[predictions_DF3['D3']==predictions_DF3['prediction']]['prediction'].count()\n",
    "total_test_records = predictions_DF3['prediction'].count()\n",
    "print('D3 accuracy = '+str(round(100*correct_combo/total_test_records,2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction   0.0   1.0\n",
      "D3                    \n",
      "0           3077  1389\n",
      "1            982  6007"
     ]
    }
   ],
   "source": [
    "pd.crosstab(predictions_DF3.D3,predictions_DF3.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D4 accuracy = 82.18%"
     ]
    }
   ],
   "source": [
    "predictions_DF4 = predictions_D4.select('D1','D2','D3','D4','D5','prediction').toPandas()\n",
    "correct_combo = predictions_DF4[predictions_DF4['D4']==predictions_DF4['prediction']]['prediction'].count()\n",
    "total_test_records = predictions_DF4['prediction'].count()\n",
    "print('D4 accuracy = '+str(round(100*correct_combo/total_test_records,2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction   0.0   1.0\n",
      "D4                    \n",
      "0           5795   770\n",
      "1           1271  3619"
     ]
    }
   ],
   "source": [
    "pd.crosstab(predictions_DF4.D4,predictions_DF4.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D5 accuracy = 77.96%"
     ]
    }
   ],
   "source": [
    "predictions_DF5 = predictions_D5.select('D1','D2','D3','D4','D5','prediction').toPandas()\n",
    "correct_combo = predictions_DF5[predictions_DF5['D5']==predictions_DF5['prediction']]['prediction'].count()\n",
    "total_test_records = predictions_DF5['prediction'].count()\n",
    "print('D5 accuracy = '+str(round(100*correct_combo/total_test_records,2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction   0.0   1.0\n",
      "D5                    \n",
      "0           6107   922\n",
      "1           1603  2823"
     ]
    }
   ],
   "source": [
    "pd.crosstab(predictions_DF5.D5,predictions_DF5.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
