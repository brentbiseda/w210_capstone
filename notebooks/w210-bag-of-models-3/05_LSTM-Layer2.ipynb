{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from pydot) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "#import torch\n",
    "#from transformers import AutoTokenizer, AutoModel\n",
    "#import re\n",
    "#import string\n",
    "import numpy as np\n",
    "import pydot\n",
    "#from nltk.corpus import stopwords \n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "#from simpletransformers.classification import MultiLabelClassificationModel\n",
    "#import logging\n",
    "#import custom_sentence_tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy import stats\n",
    "#from ast import literal_eval\n",
    "import pickle\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import gc\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, Reshape\n",
    "from keras.layers import GlobalMaxPooling1D,Conv2D, Conv1D, AveragePooling2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "#from sklearn.decomposition import PCA\n",
    "import _pickle as cPickle\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Embeddings (X) and Labels (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.7971134185791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.time()\n",
    "infile = None\n",
    "infile = open(\"embeddings_7\",'rb')\n",
    "embeddings = cPickle.load(infile)\n",
    "infile.close()\n",
    "del(infile)\n",
    "print(time.time()-now)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.72156071662903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.time()\n",
    "infile = None\n",
    "infile = open(\"embeddings_evaluation\",'rb')\n",
    "embeddings_evaluation = cPickle.load(infile)\n",
    "infile.close()\n",
    "del(infile)\n",
    "print(time.time()-now)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.update(embeddings_evaluation)\n",
    "pickle.dump( embeddings, open( \"/home/ec2-user/SageMaker/embeddings.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "infile = None\n",
    "infile = open(\"embeddings.p\",'rb')\n",
    "embeddings = cPickle.load(infile)\n",
    "infile.close()\n",
    "del(infile)\n",
    "print(time.time()-now)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np_complete = np.array(list(embeddings.items()))\n",
    "del(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load chapter model and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load chapeter model weights\n",
    "chapter_model = None\n",
    "json_file = open('model_num_cnn2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "chapter_model = model_from_json(loaded_model_json)\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "# load weights into new model\n",
    "chapter_model.load_weights(\"model_num_cnn2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(embeddings):\n",
    "    max_sentence = 20\n",
    "    padding = max_sentence - len(embeddings)\n",
    "    if padding > 0:\n",
    "        padding_shape = (padding, 256, 768)\n",
    "        pad = np.zeros(padding*256*768).reshape(padding_shape)\n",
    "        return np.append(embeddings, pad, axis = 0).astype('float16')\n",
    "    else:\n",
    "        return embeddings\n",
    "    \n",
    "def split_for_train(HADM_ID_LIST, CHAPTER_PIVOT_DF, embeddings_concat):\n",
    "    HADM_ID_DF = None\n",
    "    loaded_model = None\n",
    "    HADM_ID_DF =  pd.DataFrame(data=HADM_ID_LIST, columns=['HADM_ID'])\n",
    "    CHAPTER_PIVOT_DF_COPY = HADM_ID_DF.set_index('HADM_ID')\\\n",
    "                    .join(CHAPTER_PIVOT_DF, how='left').copy(deep=False)\n",
    "    CHAPTER_PIVOT_NP = np.array(CHAPTER_PIVOT_DF_COPY)\n",
    "    return train_test_split(embeddings_concat, CHAPTER_PIVOT_NP, test_size=0.2)\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def get_model(label_len):\n",
    "    #define model - STATIC LAYERS\n",
    "    model = Sequential()\n",
    "    for layer in chapter_model.layers[:4]:\n",
    "        layer.trainable = False\n",
    "        model.add(layer)\n",
    "    \n",
    "    #LEARNING LAYERS\n",
    "    if label_len <=4:\n",
    "        print('model 1')\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(label_len, activation='sigmoid'))     \n",
    "    elif label_len >4:\n",
    "        print('model 2')\n",
    "        #model.add(Conv1D(label_len, 4))\n",
    "        #model.add(AveragePooling1D(pool_size=2))\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        #model.add(Dropout(0.3))\n",
    "        model.add(Dense(label_len, activation='sigmoid'))   \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loop on individual therapy area (bag of models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============Started 001_139.csv============\n",
      "001_139.csv orginal shape: (5135, 2)\n",
      "001_139.csv Padding complete\n",
      "001_139.csv modified shape: (5135, 5120, 768)\n",
      "Train and Test Shapre: (4108, 5120, 768), (1027, 5120, 768), (4108, 4), (1027, 4)\n",
      "model 1\n",
      "Train on 4108 samples, validate on 256 samples\n",
      "Epoch 1/5\n",
      "4108/4108 [==============================] - 161s 39ms/step - loss: 0.5635 - get_f1: 0.5742 - accuracy: 0.7436 - val_loss: 0.4979 - val_get_f1: 0.5951 - val_accuracy: 0.7764\n",
      "Epoch 2/5\n",
      "4108/4108 [==============================] - 160s 39ms/step - loss: 0.4808 - get_f1: 0.6283 - accuracy: 0.7842 - val_loss: 0.4861 - val_get_f1: 0.6120 - val_accuracy: 0.7803\n",
      "Epoch 3/5\n",
      "4108/4108 [==============================] - 161s 39ms/step - loss: 0.4351 - get_f1: 0.6764 - accuracy: 0.8095 - val_loss: 0.4902 - val_get_f1: 0.6212 - val_accuracy: 0.7881\n",
      "Epoch 4/5\n",
      "4108/4108 [==============================] - 160s 39ms/step - loss: 0.3895 - get_f1: 0.7156 - accuracy: 0.8323 - val_loss: 0.5012 - val_get_f1: 0.6044 - val_accuracy: 0.7764\n",
      "Epoch 5/5\n",
      "4108/4108 [==============================] - 161s 39ms/step - loss: 0.3409 - get_f1: 0.7569 - accuracy: 0.8547 - val_loss: 0.5121 - val_get_f1: 0.6198 - val_accuracy: 0.7812\n",
      "Model evaluation 001_139.csv\n",
      "1027/1027 [==============================] - 38s 37ms/step\n",
      "[0.5017337987694559, 0.6196596622467041, 0.7867575287818909]\n",
      "============completed 001_139.csv============\n",
      "\n",
      "\n",
      "============Started 140_239.csv============\n",
      "140_239.csv orginal shape: (2369, 2)\n",
      "140_239.csv Padding complete\n",
      "140_239.csv modified shape: (2369, 5120, 768)\n",
      "Train and Test Shapre: (1895, 5120, 768), (474, 5120, 768), (1895, 3), (474, 3)\n",
      "model 1\n",
      "Train on 1895 samples, validate on 118 samples\n",
      "Epoch 1/5\n",
      "1895/1895 [==============================] - 74s 39ms/step - loss: 0.6257 - get_f1: 0.6195 - accuracy: 0.6767 - val_loss: 0.5346 - val_get_f1: 0.7252 - val_accuracy: 0.7542\n",
      "Epoch 2/5\n",
      "1895/1895 [==============================] - 74s 39ms/step - loss: 0.4993 - get_f1: 0.7271 - accuracy: 0.7622 - val_loss: 0.5307 - val_get_f1: 0.7186 - val_accuracy: 0.7514\n",
      "Epoch 3/5\n",
      "1895/1895 [==============================] - 74s 39ms/step - loss: 0.4446 - get_f1: 0.7579 - accuracy: 0.7924 - val_loss: 0.5340 - val_get_f1: 0.7014 - val_accuracy: 0.7458\n",
      "Epoch 4/5\n",
      "1895/1895 [==============================] - 74s 39ms/step - loss: 0.3985 - get_f1: 0.7972 - accuracy: 0.8250 - val_loss: 0.5579 - val_get_f1: 0.7025 - val_accuracy: 0.7458\n",
      "Epoch 5/5\n",
      "1895/1895 [==============================] - 74s 39ms/step - loss: 0.3521 - get_f1: 0.8235 - accuracy: 0.8485 - val_loss: 0.5414 - val_get_f1: 0.7304 - val_accuracy: 0.7712\n",
      "Model evaluation 140_239.csv\n",
      "474/474 [==============================] - 17s 37ms/step\n",
      "[0.49783752751752797, 0.7317764163017273, 0.7756681442260742]\n",
      "============completed 140_239.csv============\n",
      "\n",
      "\n",
      "============Started 240_279.csv============\n",
      "240_279.csv orginal shape: (6263, 2)\n",
      "240_279.csv Padding complete\n",
      "240_279.csv modified shape: (6263, 5120, 768)\n",
      "Train and Test Shapre: (5010, 5120, 768), (1253, 5120, 768), (5010, 4), (1253, 4)\n",
      "model 1\n",
      "Train on 5010 samples, validate on 313 samples\n",
      "Epoch 1/5\n",
      "5010/5010 [==============================] - 194s 39ms/step - loss: 0.6090 - get_f1: 0.5316 - accuracy: 0.6865 - val_loss: 0.5560 - val_get_f1: 0.5232 - val_accuracy: 0.7188\n",
      "Epoch 2/5\n",
      "5010/5010 [==============================] - 196s 39ms/step - loss: 0.5375 - get_f1: 0.5776 - accuracy: 0.7360 - val_loss: 0.5506 - val_get_f1: 0.5201 - val_accuracy: 0.7204\n",
      "Epoch 3/5\n",
      "5010/5010 [==============================] - 195s 39ms/step - loss: 0.4989 - get_f1: 0.6125 - accuracy: 0.7565 - val_loss: 0.5518 - val_get_f1: 0.5065 - val_accuracy: 0.7125\n",
      "Epoch 4/5\n",
      "5010/5010 [==============================] - 195s 39ms/step - loss: 0.4587 - get_f1: 0.6579 - accuracy: 0.7832 - val_loss: 0.5491 - val_get_f1: 0.5399 - val_accuracy: 0.7204\n",
      "Epoch 5/5\n",
      "5010/5010 [==============================] - 195s 39ms/step - loss: 0.4168 - get_f1: 0.6975 - accuracy: 0.8068 - val_loss: 0.5681 - val_get_f1: 0.5466 - val_accuracy: 0.7268\n",
      "Model evaluation 240_279.csv\n",
      "1253/1253 [==============================] - 45s 36ms/step\n",
      "[0.567120153632339, 0.5641987323760986, 0.7340382933616638]\n",
      "============completed 240_279.csv============\n",
      "\n",
      "\n",
      "============Started 280_289.csv============\n",
      "280_289.csv orginal shape: (6212, 2)\n",
      "280_289.csv Padding complete\n",
      "280_289.csv modified shape: (6212, 5120, 768)\n",
      "Train and Test Shapre: (4969, 5120, 768), (1243, 5120, 768), (4969, 3), (1243, 3)\n",
      "model 1\n",
      "Train on 4969 samples, validate on 310 samples\n",
      "Epoch 1/5\n",
      "4969/4969 [==============================] - 194s 39ms/step - loss: 0.5722 - get_f1: 0.5211 - accuracy: 0.7016 - val_loss: 0.5115 - val_get_f1: 0.5913 - val_accuracy: 0.7409\n",
      "Epoch 2/5\n",
      "4969/4969 [==============================] - 194s 39ms/step - loss: 0.4985 - get_f1: 0.6139 - accuracy: 0.7538 - val_loss: 0.5072 - val_get_f1: 0.6159 - val_accuracy: 0.7452\n",
      "Epoch 3/5\n",
      "4969/4969 [==============================] - 194s 39ms/step - loss: 0.4393 - get_f1: 0.6818 - accuracy: 0.7954 - val_loss: 0.5154 - val_get_f1: 0.5827 - val_accuracy: 0.7387\n",
      "Epoch 4/5\n",
      "4969/4969 [==============================] - 194s 39ms/step - loss: 0.3836 - get_f1: 0.7327 - accuracy: 0.8282 - val_loss: 0.5332 - val_get_f1: 0.5871 - val_accuracy: 0.7387\n",
      "Epoch 5/5\n",
      "4969/4969 [==============================] - 194s 39ms/step - loss: 0.3302 - get_f1: 0.7816 - accuracy: 0.8581 - val_loss: 0.5423 - val_get_f1: 0.6223 - val_accuracy: 0.7548\n",
      "Model evaluation 280_289.csv\n",
      "1243/1243 [==============================] - 46s 37ms/step\n",
      "[0.534071930214367, 0.6175639629364014, 0.7514079809188843]\n",
      "============completed 280_289.csv============\n",
      "\n",
      "\n",
      "============Started 290_319.csv============\n",
      "290_319.csv orginal shape: (5946, 2)\n",
      "290_319.csv Padding complete\n",
      "290_319.csv modified shape: (5946, 5120, 768)\n",
      "Train and Test Shapre: (4756, 5120, 768), (1190, 5120, 768), (4756, 4), (1190, 4)\n",
      "model 1\n",
      "Train on 4756 samples, validate on 297 samples\n",
      "Epoch 1/5\n",
      "4756/4756 [==============================] - 186s 39ms/step - loss: 0.6070 - get_f1: 0.2485 - accuracy: 0.6904 - val_loss: 0.5575 - val_get_f1: 0.0658 - val_accuracy: 0.7273\n",
      "Epoch 2/5\n",
      "4756/4756 [==============================] - 187s 39ms/step - loss: 0.5488 - get_f1: 0.3247 - accuracy: 0.7262 - val_loss: 0.5622 - val_get_f1: 0.1579 - val_accuracy: 0.7222\n",
      "Epoch 3/5\n",
      "4756/4756 [==============================] - 186s 39ms/step - loss: 0.5071 - get_f1: 0.4259 - accuracy: 0.7554 - val_loss: 0.5666 - val_get_f1: 0.1766 - val_accuracy: 0.7172\n",
      "Epoch 4/5\n",
      "4756/4756 [==============================] - 188s 39ms/step - loss: 0.4633 - get_f1: 0.5050 - accuracy: 0.7809 - val_loss: 0.5760 - val_get_f1: 0.2224 - val_accuracy: 0.7189\n",
      "Epoch 5/5\n",
      "4756/4756 [==============================] - 187s 39ms/step - loss: 0.4157 - get_f1: 0.5893 - accuracy: 0.8109 - val_loss: 0.5902 - val_get_f1: 0.2588 - val_accuracy: 0.7079\n",
      "Model evaluation 290_319.csv\n",
      "1190/1190 [==============================] - 44s 37ms/step\n",
      "[0.5699708663114981, 0.306727796792984, 0.7268907427787781]\n",
      "============completed 290_319.csv============\n",
      "\n",
      "\n",
      "============Started 320_389.csv============\n",
      "320_389.csv orginal shape: (4210, 2)\n",
      "320_389.csv Padding complete\n",
      "320_389.csv modified shape: (4210, 5120, 768)\n",
      "Train and Test Shapre: (3368, 5120, 768), (842, 5120, 768), (3368, 6), (842, 6)\n",
      "model 2\n",
      "Train on 3368 samples, validate on 210 samples\n",
      "Epoch 1/5\n",
      "3368/3368 [==============================] - 133s 40ms/step - loss: 0.5373 - get_f1: 0.1459 - accuracy: 0.7599 - val_loss: 0.4650 - val_get_f1: 0.1105 - val_accuracy: 0.8016\n",
      "Epoch 2/5\n",
      "3368/3368 [==============================] - 132s 39ms/step - loss: 0.4644 - get_f1: 0.2149 - accuracy: 0.8012 - val_loss: 0.4487 - val_get_f1: 0.2476 - val_accuracy: 0.8111\n",
      "Epoch 3/5\n",
      "3368/3368 [==============================] - 133s 39ms/step - loss: 0.4220 - get_f1: 0.3202 - accuracy: 0.8143 - val_loss: 0.4532 - val_get_f1: 0.2156 - val_accuracy: 0.8048\n",
      "Epoch 4/5\n",
      "3368/3368 [==============================] - 133s 39ms/step - loss: 0.3796 - get_f1: 0.4218 - accuracy: 0.8319 - val_loss: 0.4607 - val_get_f1: 0.2909 - val_accuracy: 0.7897\n",
      "Epoch 5/5\n",
      "3368/3368 [==============================] - 133s 40ms/step - loss: 0.3362 - get_f1: 0.5336 - accuracy: 0.8547 - val_loss: 0.4601 - val_get_f1: 0.2961 - val_accuracy: 0.7921\n",
      "Model evaluation 320_389.csv\n",
      "842/842 [==============================] - 31s 37ms/step\n",
      "[0.45049474167144216, 0.3215944766998291, 0.7975058555603027]\n",
      "============completed 320_389.csv============\n",
      "\n",
      "\n",
      "============Started 390_459.csv============\n",
      "390_459.csv orginal shape: (10831, 2)\n",
      "390_459.csv Padding complete\n",
      "390_459.csv modified shape: (10831, 5120, 768)\n",
      "Train and Test Shapre: (8664, 5120, 768), (2167, 5120, 768), (8664, 6), (2167, 6)\n",
      "model 2\n",
      "Train on 8664 samples, validate on 541 samples\n",
      "Epoch 1/5\n",
      "8664/8664 [==============================] - 345s 40ms/step - loss: 0.5609 - get_f1: 0.4140 - accuracy: 0.7122 - val_loss: 0.5398 - val_get_f1: 0.4058 - val_accuracy: 0.7215\n",
      "Epoch 2/5\n",
      "8664/8664 [==============================] - 345s 40ms/step - loss: 0.5145 - get_f1: 0.4866 - accuracy: 0.7440 - val_loss: 0.5350 - val_get_f1: 0.4581 - val_accuracy: 0.7314\n",
      "Epoch 3/5\n",
      "8664/8664 [==============================] - 346s 40ms/step - loss: 0.4831 - get_f1: 0.5419 - accuracy: 0.7651 - val_loss: 0.5393 - val_get_f1: 0.4897 - val_accuracy: 0.7338\n",
      "Epoch 4/5\n",
      "8664/8664 [==============================] - 344s 40ms/step - loss: 0.4498 - get_f1: 0.5937 - accuracy: 0.7857 - val_loss: 0.5462 - val_get_f1: 0.4808 - val_accuracy: 0.7320\n",
      "Epoch 5/5\n",
      "8664/8664 [==============================] - 346s 40ms/step - loss: 0.4104 - get_f1: 0.6435 - accuracy: 0.8077 - val_loss: 0.5488 - val_get_f1: 0.4931 - val_accuracy: 0.7320\n",
      "Model evaluation 390_459.csv\n",
      "2167/2167 [==============================] - 80s 37ms/step\n",
      "[0.5363026736626495, 0.5149402618408203, 0.7459620833396912]\n",
      "============completed 390_459.csv============\n",
      "\n",
      "\n",
      "============Started 460_519.csv============\n",
      "460_519.csv orginal shape: (9021, 2)\n",
      "460_519.csv Padding complete\n",
      "460_519.csv modified shape: (9021, 5120, 768)\n",
      "Train and Test Shapre: (7216, 5120, 768), (1805, 5120, 768), (7216, 5), (1805, 5)\n",
      "model 2\n",
      "Train on 7216 samples, validate on 451 samples\n",
      "Epoch 1/5\n",
      "7216/7216 [==============================] - 287s 40ms/step - loss: 0.5757 - get_f1: 0.3439 - accuracy: 0.7078 - val_loss: 0.5298 - val_get_f1: 0.3838 - val_accuracy: 0.7468\n",
      "Epoch 2/5\n",
      "7216/7216 [==============================] - 288s 40ms/step - loss: 0.5174 - get_f1: 0.4278 - accuracy: 0.7494 - val_loss: 0.5221 - val_get_f1: 0.4530 - val_accuracy: 0.7490\n",
      "Epoch 3/5\n",
      "7216/7216 [==============================] - 288s 40ms/step - loss: 0.4803 - get_f1: 0.4989 - accuracy: 0.7701 - val_loss: 0.5294 - val_get_f1: 0.4747 - val_accuracy: 0.7512\n",
      "Epoch 4/5\n",
      "7216/7216 [==============================] - 289s 40ms/step - loss: 0.4384 - get_f1: 0.5657 - accuracy: 0.7929 - val_loss: 0.5352 - val_get_f1: 0.4587 - val_accuracy: 0.7494\n",
      "Epoch 5/5\n",
      "7216/7216 [==============================] - 288s 40ms/step - loss: 0.3938 - get_f1: 0.6339 - accuracy: 0.8190 - val_loss: 0.5552 - val_get_f1: 0.4615 - val_accuracy: 0.7428\n",
      "Model evaluation 460_519.csv\n",
      "1805/1805 [==============================] - 67s 37ms/step\n",
      "[0.5483679038831072, 0.447293758392334, 0.7409417629241943]\n",
      "============completed 460_519.csv============\n",
      "\n",
      "\n",
      "============Started 520_579.csv============\n",
      "520_579.csv orginal shape: (4512, 2)\n",
      "520_579.csv Padding complete\n",
      "520_579.csv modified shape: (4512, 5120, 768)\n",
      "Train and Test Shapre: (3609, 5120, 768), (903, 5120, 768), (3609, 5), (903, 5)\n",
      "model 2\n",
      "Train on 3609 samples, validate on 225 samples\n",
      "Epoch 1/5\n",
      "3609/3609 [==============================] - 144s 40ms/step - loss: 0.5117 - get_f1: 0.3740 - accuracy: 0.7704 - val_loss: 0.4317 - val_get_f1: 0.5854 - val_accuracy: 0.8160\n",
      "Epoch 2/5\n",
      "3609/3609 [==============================] - 143s 40ms/step - loss: 0.4295 - get_f1: 0.5152 - accuracy: 0.8131 - val_loss: 0.4088 - val_get_f1: 0.5944 - val_accuracy: 0.8169\n",
      "Epoch 3/5\n",
      "3609/3609 [==============================] - 144s 40ms/step - loss: 0.3781 - get_f1: 0.5868 - accuracy: 0.8346 - val_loss: 0.4103 - val_get_f1: 0.5572 - val_accuracy: 0.8107\n",
      "Epoch 4/5\n",
      "3609/3609 [==============================] - 143s 40ms/step - loss: 0.3297 - get_f1: 0.6540 - accuracy: 0.8574 - val_loss: 0.4173 - val_get_f1: 0.6225 - val_accuracy: 0.8178\n",
      "Epoch 5/5\n",
      "3609/3609 [==============================] - 144s 40ms/step - loss: 0.2774 - get_f1: 0.7286 - accuracy: 0.8832 - val_loss: 0.4205 - val_get_f1: 0.6081 - val_accuracy: 0.8142\n",
      "Model evaluation 520_579.csv\n",
      "903/903 [==============================] - 34s 37ms/step\n",
      "[0.4314207401980593, 0.5716726779937744, 0.8190475106239319]\n",
      "============completed 520_579.csv============\n",
      "\n",
      "\n",
      "============Started 580_629.csv============\n",
      "580_629.csv orginal shape: (8339, 2)\n",
      "580_629.csv Padding complete\n",
      "580_629.csv modified shape: (8339, 5120, 768)\n",
      "Train and Test Shapre: (6671, 5120, 768), (1668, 5120, 768), (6671, 3), (1668, 3)\n",
      "model 1\n",
      "Train on 6671 samples, validate on 417 samples\n",
      "Epoch 1/5\n",
      "6671/6671 [==============================] - 261s 39ms/step - loss: 0.6605 - get_f1: 0.5585 - accuracy: 0.6154 - val_loss: 0.6143 - val_get_f1: 0.6391 - val_accuracy: 0.6715\n",
      "Epoch 2/5\n",
      "6671/6671 [==============================] - 261s 39ms/step - loss: 0.5888 - get_f1: 0.6436 - accuracy: 0.6869 - val_loss: 0.6085 - val_get_f1: 0.6441 - val_accuracy: 0.6707\n",
      "Epoch 3/5\n",
      "6671/6671 [==============================] - 262s 39ms/step - loss: 0.5420 - get_f1: 0.6874 - accuracy: 0.7197 - val_loss: 0.6204 - val_get_f1: 0.6391 - val_accuracy: 0.6675\n",
      "Epoch 4/5\n",
      " 928/6671 [===>..........................] - ETA: 3:29 - loss: 0.4812 - get_f1: 0.7464 - accuracy: 0.7705"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "history_list = []\n",
    "file_name_LIST = ['001_139.csv','140_239.csv','240_279.csv','280_289.csv','290_319.csv','320_389.csv',\n",
    "                  '390_459.csv', '460_519.csv','520_579.csv','580_629.csv']\n",
    "#,'460_519.csv','520_579.csv','580_629.csv','710-739.csv','780-799.csv','800-999.csv','E_V.csv']\n",
    "#'520_579.csv','580_629.csv','630_679__740_759__760_779.csv','710-739.csv','780-799.csv','800-999.csv','E_V.csv']\n",
    "1. #file_name_LIST = ['001_139.csv','140_239.csv','240_279.csv','800-999.csv']\n",
    "#file_name_LIST = ['240_279_1.csv']\n",
    "#file_name_LIST = ['280_289.csv','290_319.csv','320_389.csv','390_459.csv','460_519.csv','520_579.csv','580_629.csv']\n",
    "\n",
    "for file_name in file_name_LIST:\n",
    "    print('\\n\\n============Started '+file_name+'============')\n",
    "    CHAPTER_PIVOT_DF = pd.read_csv('Layer-2/'+file_name, sep=',', header = 0)\\\n",
    "                        .astype({'HADM_ID': 'str'}).set_index('HADM_ID')\n",
    "    HADM_ID_LIST = list(CHAPTER_PIVOT_DF.index)\n",
    "    embeddings_np = embeddings_np_complete[np.isin(embeddings_np_complete[:,0], HADM_ID_LIST)]\n",
    "    print(file_name+' orginal shape: '+str(embeddings_np.shape))\n",
    "    HADM_ID_LIST = embeddings_np[:,0].copy()\n",
    "    embeddings = embeddings_np[:,1].copy()\n",
    "    \n",
    "    #Zero Pad embeddings\n",
    "    embeddings_padded = [padding(embeddings_ITEM) for embeddings_ITEM in embeddings]\n",
    "    del(embeddings)\n",
    "    gc.collect()\n",
    "    print(file_name+' Padding complete')\n",
    "\n",
    "    #Reshape embeddings\n",
    "    #embeddings_concat = np.array([np.concatenate(i) for i in embeddings_padded])\n",
    "    embeddings_concat = np.array(embeddings_padded).reshape(len(embeddings_padded), 20*256, 768)\n",
    "    print(file_name+' modified shape: '+str(embeddings_concat.shape))\n",
    "    del(embeddings_padded)\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train2, X_test2, y_train2, y_test2 = split_for_train(HADM_ID_LIST, CHAPTER_PIVOT_DF, embeddings_concat)\n",
    "    print('Train and Test Shapre: '+str(X_train2.shape)+', '+str(X_test2.shape)+', '+str(y_train2.shape)+', '+str(y_test2.shape))\n",
    "    X_val = X_test2[:int(0.25*len(X_test2))]\n",
    "    y_val = y_test2[:int(0.25*len(y_test2))]\n",
    "    del(HADM_ID_LIST)\n",
    "    del(CHAPTER_PIVOT_DF)\n",
    "    del(embeddings_concat)\n",
    "    gc.collect()\n",
    "    \n",
    "    #call model\n",
    "    label_len = y_train2.shape[1]\n",
    "    model = get_model(label_len)\n",
    "    opt = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1, 'accuracy'])\n",
    "    history = model.fit(X_train2, y_train2, epochs=4,batch_size=32, verbose=1, validation_data = (X_val,y_val))\n",
    "    \n",
    "    #evaluate model\n",
    "    print('Model evaluation '+file_name)\n",
    "    score = model.evaluate(X_test2, y_test2, verbose=1)\n",
    "    print(score)\n",
    "    \n",
    "    #save model evaluation and history metrics\n",
    "    score_list.append(score)\n",
    "    history_list.append(history.history)\n",
    "    \n",
    "    #save model\n",
    "    model_json = model.to_json()\n",
    "    with open(\"/home/ec2-user/SageMaker/Models/model_\"+file_name+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"/home/ec2-user/SageMaker/Models/model_\"+file_name+\".h5\")\n",
    "    print('============completed '+file_name+'============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'001_139.csv','140_239.csv','240_279.csv','800-999.csv'\n",
    "# Run 800-999 and 240-279 with more epochs: 6 and 10\n",
    "pickle.dump( score_list, open( \"/home/ec2-user/SageMaker/score_list_1.p\", \"wb\" ) )\n",
    "pickle.dump( history_list, open( \"/home/ec2-user/SageMaker/history_list_1.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "history_list = []\n",
    "file_name_LIST = ['adverse_effect.csv']\n",
    "#,'460_519.csv','520_579.csv','580_629.csv','710-739.csv','780-799.csv','800-999.csv','E_V.csv']\n",
    "#'520_579.csv','580_629.csv','630_679__740_759__760_779.csv','710-739.csv','780-799.csv','800-999.csv','E_V.csv']\n",
    "1. #file_name_LIST = ['001_139.csv','140_239.csv','240_279.csv','800-999.csv']\n",
    "#file_name_LIST = ['240_279_1.csv']\n",
    "#file_name_LIST = ['280_289.csv','290_319.csv','320_389.csv','390_459.csv','460_519.csv','520_579.csv','580_629.csv']\n",
    "\n",
    "for file_name in file_name_LIST:\n",
    "    print('\\n\\n============Started '+file_name+'============')\n",
    "    CHAPTER_PIVOT_DF = pd.read_csv('Layer-2/'+file_name, sep=',', header = 0)\\\n",
    "                        .astype({'HADM_ID': 'str'}).set_index('HADM_ID')\n",
    "    HADM_ID_LIST = list(CHAPTER_PIVOT_DF.index)\n",
    "    embeddings_np = embeddings_np_complete[np.isin(embeddings_np_complete[:,0], HADM_ID_LIST)]\n",
    "    print(file_name+' orginal shape: '+str(embeddings_np.shape))\n",
    "    HADM_ID_LIST = embeddings_np[:,0].copy()\n",
    "    embeddings = embeddings_np[:,1].copy()\n",
    "    \n",
    "    #Zero Pad embeddings\n",
    "    embeddings_padded = [padding(embeddings_ITEM) for embeddings_ITEM in embeddings]\n",
    "    del(embeddings)\n",
    "    gc.collect()\n",
    "    print(file_name+' Padding complete')\n",
    "\n",
    "    #Reshape embeddings\n",
    "    #embeddings_concat = np.array([np.concatenate(i) for i in embeddings_padded])\n",
    "    embeddings_concat = np.array(embeddings_padded).reshape(len(embeddings_padded), 20*256, 768)\n",
    "    print(file_name+' modified shape: '+str(embeddings_concat.shape))\n",
    "    del(embeddings_padded)\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train2, X_test2, y_train2, y_test2 = split_for_train(HADM_ID_LIST, CHAPTER_PIVOT_DF, embeddings_concat)\n",
    "    print('Train and Test Shapre: '+str(X_train2.shape)+', '+str(X_test2.shape)+', '+str(y_train2.shape)+', '+str(y_test2.shape))\n",
    "    X_val = X_test2[:int(0.25*len(X_test2))]\n",
    "    y_val = y_test2[:int(0.25*len(y_test2))]\n",
    "    del(HADM_ID_LIST)\n",
    "    del(CHAPTER_PIVOT_DF)\n",
    "    del(embeddings_concat)\n",
    "    gc.collect()\n",
    "    \n",
    "    #call model\n",
    "    label_len = y_train2.shape[1]\n",
    "    model = get_model(label_len)\n",
    "    opt = keras.optimizers.Adam(lr=0.00001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1, 'accuracy'])\n",
    "    history = model.fit(X_train2, y_train2, epochs=5,batch_size=32, verbose=1, \n",
    "                        validation_data = (X_val,y_val))\n",
    "    \n",
    "    #evaluate model\n",
    "    print('Model evaluation '+file_name)\n",
    "    score = model.evaluate(X_test2, y_test2, verbose=1)\n",
    "    print(score)\n",
    "    \n",
    "    #save model evaluation and history metrics\n",
    "    score_list.append(score)\n",
    "    history_list.append(history.history)\n",
    "    \n",
    "    #save model\n",
    "    model_json = model.to_json()\n",
    "    with open(\"/home/ec2-user/SageMaker/Models/model_\"+file_name+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"/home/ec2-user/SageMaker/Models/model_\"+file_name+\".h5\")\n",
    "    print('============completed '+file_name+'============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(label_len):\n",
    "    #define model - STATIC LAYERS\n",
    "    model = Sequential()\n",
    "    for layer in chapter_model.layers[:4]:\n",
    "        layer.trainable = False\n",
    "        model.add(layer)\n",
    "    \n",
    "    #LEARNING LAYERS\n",
    "    if label_len <4:\n",
    "        print('model 1')\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(label_len, activation='sigmoid'))     \n",
    "    elif label_len >=4:\n",
    "        print('model 2')\n",
    "        #model.add(Conv1D(label_len, 4))\n",
    "        #model.add(AveragePooling1D(pool_size=2))\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        #model.add(Dropout(0.3))\n",
    "        model.add(Dense(label_len, activation='sigmoid'))   \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_len = y_train2.shape[1]\n",
    "model = get_model(label_len)\n",
    "opt = keras.optimizers.Adam(lr=0.00001)\n",
    "model.compile(loss=f1_loss, optimizer=opt, metrics=[get_f1, 'accuracy'])\n",
    "history = model.fit(X_train2, y_train2, epochs=5,batch_size=32, verbose=1, \n",
    "                        validation_data = (X_val,y_val))\n",
    "    \n",
    "#evaluate model\n",
    "print('Model evaluation '+file_name)\n",
    "score = model.evaluate(X_test2, y_test2, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5071/5071 [==============================] - 183s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_model(label_len):\n",
    "    #define model - STATIC LAYERS\n",
    "    model = Sequential()\n",
    "    for layer in chapter_model.layers[:4]:\n",
    "        layer.trainable = False\n",
    "        model.add(layer)\n",
    "    return model\n",
    "\n",
    "label_len = y_train2.shape[1]\n",
    "model = get_model(label_len)\n",
    "opt = keras.optimizers.Adam(lr=0.00001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1, 'accuracy'])\n",
    "flat_train = model.predict(X_train2, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5071, 20448)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=10, n_jobs=None, oob_score=False,\n",
       "                      random_state=42, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 10, random_state = 42, verbose = 1)\n",
    "rf.fit(flat_train, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "predictions = rf.predict(flat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5071, 4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[predictions>0.5] = 1\n",
    "predictions[predictions<=0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97070317724093"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.f1_score(y_train2, predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1268/1268 [==============================] - 46s 37ms/step\n"
     ]
    }
   ],
   "source": [
    "label_len = y_test2.shape[1]\n",
    "model = get_model(label_len)\n",
    "opt = keras.optimizers.Adam(lr=0.00001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1, 'accuracy'])\n",
    "flat_test = model.predict(X_test2, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2815310105517057"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = rf.predict(flat_test)\n",
    "predictions[predictions>0.5] = 1\n",
    "predictions[predictions<=0.5] = 0\n",
    "sklearn.metrics.f1_score(y_test2, predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
