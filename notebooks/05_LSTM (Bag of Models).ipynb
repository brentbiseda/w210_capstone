{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import torch\n",
    "#from transformers import AutoTokenizer, AutoModel\n",
    "#import re\n",
    "#import string\n",
    "import numpy as np\n",
    "#from nltk.corpus import stopwords \n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "#from simpletransformers.classification import MultiLabelClassificationModel\n",
    "#import logging\n",
    "#import custom_sentence_tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy import stats\n",
    "#from ast import literal_eval\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, Reshape\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "#from sklearn.decomposition import PCA\n",
    "import _pickle as cPickle\n",
    "import io\n",
    "import time\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip Files of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile('chapter_labels.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('chapter_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadFromS3(i):\n",
    "    # https://w210-mimic.s3.amazonaws.com/embeddings1\n",
    "    now = time.time()\n",
    "    print(\"Starting Load for File\",i)\n",
    "    embeddings = {}\n",
    "    session = boto3.session.Session(region_name='us-east-1')\n",
    "    s3client = session.client('s3')\n",
    "    response = s3client.get_object(Bucket='w210-mimic', Key=i)\n",
    "    body_string = response['Body'].read()\n",
    "    embeddings.update(cPickle.loads(body_string))\n",
    "    with open(i, 'wb') as handle:\n",
    "        pickle.dump(embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"File Loaded in:\", time.time()-now)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveToS3(file, name):\n",
    "    # https://w210-mimic.s3.amazonaws.com/embeddings1\n",
    "    now = time.time()\n",
    "    print(\"Starting Write for File\", name)\n",
    "    file = file.to_json()\n",
    "    file.save(f'{name}.h5')\n",
    "    session = boto3.session.Session(region_name='us-east-1')\n",
    "    s3 = boto3.resource('s3')\n",
    "    object = s3.Object('w210-mimic', name)\n",
    "    object.put(Body=file)\n",
    "\n",
    "    print(\"File Saved in:\", time.time()-now)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the Files Locally:\n",
    "\n",
    "# model_json = model.to_json()\n",
    "\n",
    "# with open(\"adverse_num.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"adverse_num.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "\n",
    "for file in os.listdir('./chapter_labels/'):\n",
    "    LABEL_DF = pd.read_csv('./chapter_labels/'+file, sep=',', header = 0)\\\n",
    "                        .astype({'HADM_ID': 'str'}).set_index('HADM_ID')\n",
    "    \n",
    "    NODE_COUNT = len(LABEL_DF.columns)\n",
    "    \n",
    "    deep_inputs = Input(shape=(3840,768))\n",
    "    LSTM_Layer_1 = LSTM(128)(deep_inputs)\n",
    "    dense_layer_1 = Dense(NODE_COUNT, activation='sigmoid')(LSTM_Layer_1)\n",
    "    opt = keras.optimizers.Adam(lr=0.0001)\n",
    "    model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1])\n",
    "    model_list.append(model)\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    with open(f\"models/model_{file}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 460,554\n",
      "Trainable params: 460,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 459,651\n",
      "Trainable params: 459,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 460,425\n",
      "Trainable params: 460,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 460,038\n",
      "Trainable params: 460,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 21)                2709      \n",
      "=================================================================\n",
      "Total params: 461,973\n",
      "Trainable params: 461,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 460,296\n",
      "Trainable params: 460,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 459,522\n",
      "Trainable params: 459,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 460,038\n",
      "Trainable params: 460,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 460,167\n",
      "Trainable params: 460,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 459,780\n",
      "Trainable params: 459,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 459,780\n",
      "Trainable params: 459,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 11)                1419      \n",
      "=================================================================\n",
      "Total params: 460,683\n",
      "Trainable params: 460,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 461,715\n",
      "Trainable params: 461,715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 460,554\n",
      "Trainable params: 460,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        (None, 3840, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 459,780\n",
      "Trainable params: 459,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for model in model_list:\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create List of Models with outputs that tie to Chapter Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import model_from_json\n",
    "\n",
    "# json_file = open('./adverse_num.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# loaded_model.load_weights(\"./adverse_3_embeddings1.hdr\")\n",
    "\n",
    "# opt = keras.optimizers.Adam(lr=0.0001)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1])\n",
    "\n",
    "# print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Load for File embeddings6\n",
      "File Loaded in: 678.4929823875427\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 630_679__740_759__760_779.csv Epoch: 0 Embeddings: embeddings6\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 79 samples, validate on 9 samples\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "79/79 [==============================] - 49s 623ms/step - loss: 0.6905 - get_f1: 0.0099 - val_loss: 0.6677 - val_get_f1: 0.0909\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 710-739.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 607 samples, validate on 68 samples\n",
      "Epoch 1/1\n",
      "607/607 [==============================] - 45s 74ms/step - loss: 0.6550 - get_f1: 0.0999 - val_loss: 0.6127 - val_get_f1: 0.1176\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 780-799.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 1066 samples, validate on 119 samples\n",
      "Epoch 1/1\n",
      "1066/1066 [==============================] - 68s 63ms/step - loss: 0.7005 - get_f1: 0.0481 - val_loss: 0.6483 - val_get_f1: 0.0471\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 290_319.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 839 samples, validate on 94 samples\n",
      "Epoch 1/1\n",
      "839/839 [==============================] - 53s 63ms/step - loss: 0.6735 - get_f1: 0.1709 - val_loss: 0.6373 - val_get_f1: 0.1324\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: E_V.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 2453 samples, validate on 273 samples\n",
      "Epoch 1/1\n",
      "2453/2453 [==============================] - 119s 49ms/step - loss: 0.6585 - get_f1: 0.1916 - val_loss: 0.6165 - val_get_f1: 0.2459\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 520_579.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 1084 samples, validate on 121 samples\n",
      "Epoch 1/1\n",
      "1084/1084 [==============================] - 69s 64ms/step - loss: 0.7057 - get_f1: 0.1166 - val_loss: 0.6565 - val_get_f1: 0.1518\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 140_239.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 468 samples, validate on 52 samples\n",
      "Epoch 1/1\n",
      "468/468 [==============================] - 34s 73ms/step - loss: 0.6954 - get_f1: 0.0820 - val_loss: 0.6895 - val_get_f1: 0.0500\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 320_389.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 818 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "818/818 [==============================] - 57s 70ms/step - loss: 0.6680 - get_f1: 0.1212 - val_loss: 0.6208 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 460_519.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 1295 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "1295/1295 [==============================] - 84s 65ms/step - loss: 0.6981 - get_f1: 0.2838 - val_loss: 0.6585 - val_get_f1: 0.2655\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 001_139.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 831 samples, validate on 93 samples\n",
      "Epoch 1/1\n",
      "831/831 [==============================] - 57s 69ms/step - loss: 0.6645 - get_f1: 0.3212 - val_loss: 0.6074 - val_get_f1: 0.5466\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 280_289.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 1048 samples, validate on 117 samples\n",
      "Epoch 1/1\n",
      "1048/1048 [==============================] - 70s 67ms/step - loss: 0.6679 - get_f1: 0.2021 - val_loss: 0.6171 - val_get_f1: 0.1500\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 800-999.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 1008 samples, validate on 112 samples\n",
      "Epoch 1/1\n",
      "1008/1008 [==============================] - 65s 64ms/step - loss: 0.6837 - get_f1: 0.0535 - val_loss: 0.6545 - val_get_f1: 0.0474\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 390_459.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 2152 samples, validate on 240 samples\n",
      "Epoch 1/1\n",
      "2152/2152 [==============================] - 120s 56ms/step - loss: 0.6689 - get_f1: 0.2580 - val_loss: 0.6372 - val_get_f1: 0.3072\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 240_279.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 1874 samples, validate on 209 samples\n",
      "Epoch 1/1\n",
      "1874/1874 [==============================] - 111s 59ms/step - loss: 0.6632 - get_f1: 0.0408 - val_loss: 0.6230 - val_get_f1: 0.0097\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 580_629.csv Epoch: 0 Embeddings: embeddings6\n",
      "Train on 1259 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "1259/1259 [==============================] - 80s 64ms/step - loss: 0.6695 - get_f1: 0.3083 - val_loss: 0.6328 - val_get_f1: 0.3378\n",
      "Starting Load for File embeddings5\n",
      "File Loaded in: 2610.154385328293\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 630_679__740_759__760_779.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 366 samples, validate on 41 samples\n",
      "Epoch 1/1\n",
      "366/366 [==============================] - 27s 74ms/step - loss: 0.6804 - get_f1: 0.0347 - val_loss: 0.6575 - val_get_f1: 0.0286\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 710-739.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 2047 samples, validate on 228 samples\n",
      "Epoch 1/1\n",
      "2047/2047 [==============================] - 129s 63ms/step - loss: 0.5772 - get_f1: 0.0113 - val_loss: 0.5541 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 780-799.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 3933 samples, validate on 437 samples\n",
      "Epoch 1/1\n",
      "3933/3933 [==============================] - 180s 46ms/step - loss: 0.5827 - get_f1: 0.0231 - val_loss: 0.5072 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 290_319.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 3286 samples, validate on 366 samples\n",
      "Epoch 1/1\n",
      "3286/3286 [==============================] - 148s 45ms/step - loss: 0.5934 - get_f1: 0.0605 - val_loss: 0.5731 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: E_V.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 415s 46ms/step - loss: 0.5586 - get_f1: 0.4426 - val_loss: 0.5078 - val_get_f1: 0.5027\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 520_579.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 3915 samples, validate on 435 samples\n",
      "Epoch 1/1\n",
      "3915/3915 [==============================] - 183s 47ms/step - loss: 0.5962 - get_f1: 0.0496 - val_loss: 0.5263 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 140_239.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 1537 samples, validate on 171 samples\n",
      "Epoch 1/1\n",
      "1537/1537 [==============================] - 91s 59ms/step - loss: 0.6406 - get_f1: 0.0153 - val_loss: 0.5935 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 320_389.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 3213 samples, validate on 357 samples\n",
      "Epoch 1/1\n",
      "3213/3213 [==============================] - 148s 46ms/step - loss: 0.5827 - get_f1: 0.0218 - val_loss: 0.5201 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 460_519.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 4689 samples, validate on 522 samples\n",
      "Epoch 1/1\n",
      "4689/4689 [==============================] - 222s 47ms/step - loss: 0.6109 - get_f1: 0.1617 - val_loss: 0.5797 - val_get_f1: 0.0504\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 001_139.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 2760 samples, validate on 307 samples\n",
      "Epoch 1/1\n",
      "2760/2760 [==============================] - 149s 54ms/step - loss: 0.5865 - get_f1: 0.6079 - val_loss: 0.5349 - val_get_f1: 0.6713\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 280_289.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 3767 samples, validate on 419 samples\n",
      "Epoch 1/1\n",
      "3767/3767 [==============================] - 174s 46ms/step - loss: 0.5883 - get_f1: 0.1252 - val_loss: 0.5584 - val_get_f1: 0.1402\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 800-999.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 3904 samples, validate on 434 samples\n",
      "Epoch 1/1\n",
      "3904/3904 [==============================] - 180s 46ms/step - loss: 0.5950 - get_f1: 0.0365 - val_loss: 0.5321 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 390_459.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 7632 samples, validate on 848 samples\n",
      "Epoch 1/1\n",
      "7632/7632 [==============================] - 346s 45ms/step - loss: 0.5916 - get_f1: 0.3016 - val_loss: 0.5489 - val_get_f1: 0.2523\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 240_279.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 6668 samples, validate on 741 samples\n",
      "Epoch 1/1\n",
      "6668/6668 [==============================] - 304s 46ms/step - loss: 0.5522 - get_f1: 0.0017 - val_loss: 0.4961 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 580_629.csv Epoch: 0 Embeddings: embeddings5\n",
      "Train on 4245 samples, validate on 472 samples\n",
      "Epoch 1/1\n",
      "4245/4245 [==============================] - 193s 46ms/step - loss: 0.6211 - get_f1: 0.3093 - val_loss: 0.6155 - val_get_f1: 0.3395\n",
      "Starting Load for File embeddings4\n",
      "File Loaded in: 2698.760986328125\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 630_679__740_759__760_779.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 898 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "898/898 [==============================] - 51s 57ms/step - loss: 0.6819 - get_f1: 0.0307 - val_loss: 0.6657 - val_get_f1: 0.0577\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 710-739.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1\n",
      "1692/1692 [==============================] - 80s 47ms/step - loss: 0.5483 - get_f1: 0.0000e+00 - val_loss: 0.5557 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 780-799.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 3307 samples, validate on 368 samples\n",
      "Epoch 1/1\n",
      "3307/3307 [==============================] - 153s 46ms/step - loss: 0.5060 - get_f1: 0.0000e+00 - val_loss: 0.4798 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 290_319.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 2890 samples, validate on 322 samples\n",
      "Epoch 1/1\n",
      "2890/2890 [==============================] - 136s 47ms/step - loss: 0.5508 - get_f1: 0.0000e+00 - val_loss: 0.5498 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: E_V.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 8999 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8999/8999 [==============================] - 420s 47ms/step - loss: 0.5248 - get_f1: 0.4880 - val_loss: 0.4902 - val_get_f1: 0.5437\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 520_579.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 3511 samples, validate on 391 samples\n",
      "Epoch 1/1\n",
      "3511/3511 [==============================] - 164s 47ms/step - loss: 0.5458 - get_f1: 0.0000e+00 - val_loss: 0.5360 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 140_239.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 1416 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "1416/1416 [==============================] - 69s 49ms/step - loss: 0.6195 - get_f1: 0.0000e+00 - val_loss: 0.6671 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 320_389.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 2741 samples, validate on 305 samples\n",
      "Epoch 1/1\n",
      "2741/2741 [==============================] - 128s 47ms/step - loss: 0.5333 - get_f1: 0.0000e+00 - val_loss: 0.5330 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 460_519.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 4213 samples, validate on 469 samples\n",
      "Epoch 1/1\n",
      "4213/4213 [==============================] - 198s 47ms/step - loss: 0.5883 - get_f1: 0.0196 - val_loss: 0.5930 - val_get_f1: 0.0030\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 001_139.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 2332 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "2332/2332 [==============================] - 115s 49ms/step - loss: 0.5722 - get_f1: 0.6282 - val_loss: 0.5385 - val_get_f1: 0.7008\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 280_289.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 3219 samples, validate on 358 samples\n",
      "Epoch 1/1\n",
      "3219/3219 [==============================] - 152s 47ms/step - loss: 0.5809 - get_f1: 0.1801 - val_loss: 0.5788 - val_get_f1: 0.1404\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 800-999.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 3696 samples, validate on 411 samples\n",
      "Epoch 1/1\n",
      "3696/3696 [==============================] - 174s 47ms/step - loss: 0.5408 - get_f1: 0.0000e+00 - val_loss: 0.5363 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 390_459.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 6856 samples, validate on 762 samples\n",
      "Epoch 1/1\n",
      "6856/6856 [==============================] - 313s 46ms/step - loss: 0.5596 - get_f1: 0.2731 - val_loss: 0.5553 - val_get_f1: 0.2816\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 240_279.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 5752 samples, validate on 640 samples\n",
      "Epoch 1/1\n",
      "5752/5752 [==============================] - 261s 45ms/step - loss: 0.5066 - get_f1: 0.0000e+00 - val_loss: 0.4960 - val_get_f1: 0.0000e+00\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 580_629.csv Epoch: 0 Embeddings: embeddings4\n",
      "Train on 3540 samples, validate on 394 samples\n",
      "Epoch 1/1\n",
      "3540/3540 [==============================] - 165s 47ms/step - loss: 0.6182 - get_f1: 0.2988 - val_loss: 0.6385 - val_get_f1: 0.2907\n",
      "Starting Load for File embeddings3\n",
      "File Loaded in: 2229.717061519623\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 630_679__740_759__760_779.csv Epoch: 0 Embeddings: embeddings3\n",
      "Train on 1270 samples, validate on 142 samples\n",
      "Epoch 1/1\n",
      "1270/1270 [==============================] - 67s 53ms/step - loss: 0.6750 - get_f1: 0.0358 - val_loss: 0.6516 - val_get_f1: 0.0252\n",
      "Padding Embeddings:\n",
      "\n",
      "Training Model: 710-739.csv Epoch: 0 Embeddings: embeddings3\n",
      "Train on 1455 samples, validate on 162 samples\n",
      "Epoch 1/1\n",
      " 512/1455 [=========>....................] - ETA: 42s - loss: 0.5280 - get_f1: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for i in ['embeddings6', 'embeddings5', 'embeddings4', 'embeddings3', 'embeddings2', 'embeddings1']:\n",
    "        rawData = LoadFromS3(i)\n",
    "        \n",
    "        rawData = np.array(list(rawData.items()))\n",
    "        HADM_ID_LIST_RAW = rawData[:,0]\n",
    "        rawData = rawData[:,1]\n",
    "\n",
    "        \n",
    "        for model_index, file in enumerate(os.listdir('./chapter_labels/')):\n",
    "            LABEL_DF = pd.read_csv('./chapter_labels/'+file, sep=',', header = 0)\\\n",
    "                                .astype({'HADM_ID': 'str'}).set_index('HADM_ID')\n",
    "        \n",
    "            # Subset the Raw Data to only include records present in the LABEL_DF\n",
    "            matchingIndex = np.in1d(HADM_ID_LIST_RAW, LABEL_DF.index)\n",
    "            \n",
    "            data = rawData[matchingIndex]\n",
    "            HADM_ID_LIST = HADM_ID_LIST_RAW[matchingIndex]\n",
    "            del(matchingIndex)\n",
    "            \n",
    "            data = [np.float16(np.concatenate(i[:30])) for i in data]\n",
    "            embeddings_padded = []\n",
    "        \n",
    "            print(\"Padding Embeddings:\")\n",
    "            for j in data:\n",
    "                pad_len = 3840 - len(j)\n",
    "                if pad_len:\n",
    "                    embeddings_padded.append(np.append(j, np.zeros(pad_len*768, dtype=np.float16).reshape(pad_len, 768), axis=0))\n",
    "                else:\n",
    "                    embeddings_padded.append(j)\n",
    "\n",
    "            del(data)\n",
    "            embeddings_padded = np.float16(embeddings_padded)\n",
    "            embeddings_padded = np.array(embeddings_padded, dtype=np.float16)\n",
    "\n",
    "            EMBEDDINGS_CHAPTER_PIVOT_DF = pd.DataFrame(data=list(zip(HADM_ID_LIST, embeddings_padded)), columns=['HADM_ID', 'embeddings']).set_index('HADM_ID')\\\n",
    "                        .join(LABEL_DF, how='left')\n",
    "            EMBEDDINGS_CHAPTER_PIVOT_DF = EMBEDDINGS_CHAPTER_PIVOT_DF.fillna(0)\n",
    "            EMBEDDINGS_CHAPTER_PIVOT_DF = EMBEDDINGS_CHAPTER_PIVOT_DF.iloc[:,1:]\n",
    "            EMBEDDINGS_CHAPTER_PIVOT_NP = np.array(EMBEDDINGS_CHAPTER_PIVOT_DF)\n",
    "            del(EMBEDDINGS_CHAPTER_PIVOT_DF)\n",
    "\n",
    "            # Rebalance the Dataset using class weights\n",
    "    #         class_weights = {}\n",
    "    #         class_weight_vals = 1/(EMBEDDINGS_CHAPTER_PIVOT_NP.sum(axis=0)/EMBEDDINGS_CHAPTER_PIVOT_NP.shape[0])\n",
    "\n",
    "    #         for index, weight in enumerate(class_weight_vals):\n",
    "    #             class_weights[index] = int(weight)\n",
    "\n",
    "            print(f\"\\nTraining Model: {file} Epoch: {epoch} Embeddings: {i}\")\n",
    "            current_model = model_list[model_index]\n",
    "            \n",
    "            history = current_model.fit(embeddings_padded, EMBEDDINGS_CHAPTER_PIVOT_NP, epochs=1, batch_size=256, verbose=1, validation_split=0.1) # ,class_weight=class_weights)\n",
    "            current_model.save(f\"models/model_{file}_{epoch}_{i}.hdr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cells Below are for Debugging & Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation\t Concatenate\t Dense\t Dropout\t EMBEDDINGS_CHAPTER_PIVOT_NP\t EPOCHS\t Embedding\t Flatten\t GlobalMaxPooling1D\t \n",
      "HADM_ID_LIST\t HADM_ID_LIST_RAW\t Input\t K\t LABEL_DF\t LSTM\t LSTM_Layer_1\t LoadFromS3\t Model\t \n",
      "NODE_COUNT\t Reshape\t SaveToS3\t Sequential\t Tokenizer\t TransferConfig\t boto3\t cPickle\t current_model\t \n",
      "deep_inputs\t dense_layer_1\t embeddings_padded\t epoch\t file\t get_f1\t history\t i\t io\t \n",
      "j\t keras\t matchingIndex\t model\t model_index\t model_list\t np\t one_hot\t opt\t \n",
      "os\t pad_len\t pad_sequences\t pd\t pickle\t plt\t rawData\t tf\t time\t \n",
      "train_test_split\t \n"
     ]
    }
   ],
   "source": [
    "who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
